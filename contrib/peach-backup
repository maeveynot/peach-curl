#!/bin/sh -e

# Ensure that we're logged in

peach -q

# Get our ID

id="$(jq -r '.data.streams[0].id' ~/.peach-session)"

# Make sure Peach is up

if ! peach /activity/isUnread >/dev/null; then
    echo "couldn't connect to Peach! exiting."
    exit 1
fi

# Save the current pagination setting

orig_pagination="$(peach /stream/allow-pagination | jq '.data.allowPagination')"

if test "$orig_pagination" = false; then
    echo "turning on pagination so we can download old posts..."
    peach /stream/allow-pagination -d '{"allowPagination": true}' >/dev/null
fi

# Make a place to write stuff, and some temp files

output_dir="peach-backup-$id"
data_dir="$output_dir/data"
image_dir="$output_dir/images"
mkdir -p "$data_dir" "$image_dir"

date="$(date +%Y-%m-%d-%H%M)"
info="$data_dir/info-${date}.json"
posts="$data_dir/posts-backup-${date}.jsonl"

temp_dir="$(mktemp -d -t peach-backup.XXXXXX)"
cleanup() { rm -rf "$temp_dir"; }
trap 'exit $?' HUP INT QUIT TERM; trap cleanup exit

# Fetch all posts

echo "downloading text of posts..."

until test "$cursor" = 0 -o "$cursor" = null; do
    stream="$temp_dir"/stream.json
    peach "/stream/id/$id${cursor:+?cursor=$cursor}" >"$stream"

    if test "$(jq -r '.success' "$stream")" = 1; then
        cursor="$(jq -r '.data.cursor' "$stream")"
        stream_page="$temp_dir/page-${cursor:-0}.jsonl"
        jq -c '.data.posts[]' "$stream" >"$stream_page"
        jq '.data | del(.posts)' "$stream" >"$info"
    else
        echo "failed to download page! exiting."
        exit 1
    fi

    oldest_post="$(jq --raw-output --slurp 'first | .createdTime | gmtime | strftime("%F %T")' "$stream_page")"
    echo "scrolled back to $oldest_post, downloading more..."
    sleep 1
done

cat "$temp_dir"/page-*.jsonl >"$posts"

# Extract a list of all the months that we have posts for

jq --slurp \
    'flatten | sort_by(.createdTime) |
     map(.createdTime | gmtime | {key: strftime("%Y-%m"), value: strftime("%B %Y")}) |
     unique | from_entries' \
    "$data_dir"/posts-backup-*.jsonl >"$temp_dir/months.json"

# Our template is in this script's directory

template_dir="$(dirname "$(readlink -f "$0")")"
template="$template_dir/peach-backup-template.pug"

write_pug() {
    out="$1"; shift
    cat > "$temp_dir/pug-input.json"
    pug --obj "$temp_dir/pug-input.json" --pretty --path "$template" <"$template" >"$out"
    echo "wrote HTML file $(basename "$out")"
}

# Generate the index page

jq --slurpfile info "$info" \
    '{info: $info[0], months: .}' \
    "$temp_dir/months.json" |
    write_pug "$output_dir/index.html"

# Write the page for each month
#
# Note: unfortunately, times are in UTC here because older versions of jq only
# have gmtime, not localtime. This doesn't affect the per-post dates that get
# rendered in the HTML but it does mean how we split them into months here is
# wrong. I suppose we could parse `date +%z` or something but meh.

jq --raw-output 'keys[]' "$temp_dir/months.json" |
    while read -r month; do
        jq --slurpfile info "$info" --arg month "$month" --slurp \
            'flatten | map(select(.createdTime | gmtime | strftime("%Y-%m") == $month)) |
             unique | sort_by(.createdTime) | {posts: ., info: $info[0]}' \
            "$data_dir"/posts-backup-*.jsonl |
            write_pug "$output_dir/$month.html"
    done

# Restore pagination setting if necessary

if test "$orig_pagination" = false; then
    echo "turning pagination back off..."
    peach /stream/allow-pagination -d '{"allowPagination": false}' >/dev/null
fi

echo
echo "backup HTML files written to $output_dir!"
echo "open this in your web browser:"
echo
echo "    file://$(readlink -f "$output_dir")/index.html"
echo
echo "now downloading images (this may take a while)..."
echo

# Fetch all images referenced in the posts

jq --raw-output \
    --argjson types '{"image": ["src"], "gif": ["src"], "video": ["src", "posterSrc"]}' \
    '.id as $id | .message[] | . as $m | $types[.type] // empty | .[] | "\($id) \($m[.])"' \
    "$posts" |
    while read post_id image_url; do
        dir="$image_dir/$post_id"
        file="$(basename "$image_url" | sed 's/\?.*//')"
        out="$dir/$file"
        mkdir -p "$dir"
        if ! test -e "$out"; then
            echo "downloading image $(echo "$file" | sed 's/\?.*//')..."
            curl -s -o "$out" "$image_url" || echo "couldn't download <$image_url>! skipping."
            sleep 1
        fi
    done
